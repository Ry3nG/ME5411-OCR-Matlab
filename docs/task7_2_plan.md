# Task 7.2 Implementation Plan: SOM + Bag-of-Visual-Words + SVM

**Date**: November 3, 2025
**Method**: Self-Organizing Map (SOM) for codebook learning + Bag-of-Visual-Words feature extraction + Support Vector Machine (SVM) classification

---

## üìå Overview

### Motivation
Task 7.2 requires a **non-CNN-based method** using approaches **covered in Part 2 of the course**. After reviewing the course content:
- ‚úÖ **SOM (Self-Organizing Map)**: Part 2 unsupervised learning method
- ‚úÖ **SVM (Support Vector Machine)**: Part 2 supervised learning method (confirmed in course slides)
- ‚úÖ **Method combination**: Task 2 explicitly allows "combination of such methods"

### Core Idea
**Bag-of-Visual-Words (BoVW) with SOM-based Codebook**

1. **Unsupervised Learning**: Train SOM on image patches to learn a codebook of visual prototypes (visual words)
2. **Feature Encoding**: Represent each character as a histogram over the learned visual words
3. **Supervised Classification**: Train SVM on the histogram features for 7-class character recognition

This approach:
- Focuses on **local texture/stroke patterns**
- Does not require HOG (which is not in the course)
- Provides **low-dimensional features** (~64D histogram)
- Shows deep understanding of **combining unsupervised + supervised learning**

---

## üèóÔ∏è System Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    TRAINING PIPELINE                            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Step 1: Patch Sampling (Unsupervised Data Collection)
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Training     ‚îÇ
‚îÇ Images       ‚îÇ  ‚îÄ‚îÄ‚îê
‚îÇ (5,327)      ‚îÇ    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
                    ‚îú‚îÄ‚îÄ> Random sample 8√ó8 patches
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ    Collect ~50K-100K patches
‚îÇ 124√ó124      ‚îÇ    ‚îÇ    Flatten to 64D vectors
‚îÇ Grayscale    ‚îÇ  ‚îÄ‚îÄ‚îò    Normalize to [0, 1]
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                    ‚îÇ
                    ‚Üì
              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
              ‚îÇ Patch Pool   ‚îÇ
              ‚îÇ (50K √ó 64D)  ‚îÇ
              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò


Step 2: SOM Training (Learn Visual Codebook)
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Patch Pool   ‚îÇ
‚îÇ (50K √ó 64D)  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Self-Organizing Map (SOM)            ‚îÇ
‚îÇ  ‚îå‚îÄ‚î¨‚îÄ‚î¨‚îÄ‚î¨‚îÄ‚î¨‚îÄ‚î¨‚îÄ‚î¨‚îÄ‚î¨‚îÄ‚îê                    ‚îÇ
‚îÇ  ‚îú‚îÄ‚îº‚îÄ‚îº‚îÄ‚îº‚îÄ‚îº‚îÄ‚îº‚îÄ‚îº‚îÄ‚îº‚îÄ‚î§  8√ó8 grid         ‚îÇ
‚îÇ  ‚îú‚îÄ‚îº‚îÄ‚îº‚îÄ‚îº‚îÄ‚îº‚îÄ‚îº‚îÄ‚îº‚îÄ‚îº‚îÄ‚î§  = 64 neurons     ‚îÇ
‚îÇ  ‚îú‚îÄ‚îº‚îÄ‚îº‚îÄ‚îº‚îÄ‚îº‚îÄ‚îº‚îÄ‚îº‚îÄ‚îº‚îÄ‚î§                    ‚îÇ
‚îÇ  ‚îú‚îÄ‚îº‚îÄ‚îº‚îÄ‚îº‚îÄ‚îº‚îÄ‚îº‚îÄ‚îº‚îÄ‚îº‚îÄ‚î§  Each neuron:     ‚îÇ
‚îÇ  ‚îú‚îÄ‚îº‚îÄ‚îº‚îÄ‚îº‚îÄ‚îº‚îÄ‚îº‚îÄ‚îº‚îÄ‚îº‚îÄ‚î§  64D weight vector‚îÇ
‚îÇ  ‚îú‚îÄ‚îº‚îÄ‚îº‚îÄ‚îº‚îÄ‚îº‚îÄ‚îº‚îÄ‚îº‚îÄ‚îº‚îÄ‚î§  (8√ó8 prototype)  ‚îÇ
‚îÇ  ‚îú‚îÄ‚îº‚îÄ‚îº‚îÄ‚îº‚îÄ‚îº‚îÄ‚îº‚îÄ‚îº‚îÄ‚îº‚îÄ‚î§                    ‚îÇ
‚îÇ  ‚îî‚îÄ‚î¥‚îÄ‚î¥‚îÄ‚î¥‚îÄ‚î¥‚îÄ‚î¥‚îÄ‚î¥‚îÄ‚î¥‚îÄ‚îò                    ‚îÇ
‚îÇ                                        ‚îÇ
‚îÇ  Training: Competitive learning        ‚îÇ
‚îÇ  - Find BMU (Best Matching Unit)      ‚îÇ
‚îÇ  - Update BMU + neighbors             ‚îÇ
‚îÇ  - Decay learning rate & neighborhood ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 64 Visual    ‚îÇ
‚îÇ Prototypes   ‚îÇ  ‚îÄ‚îÄ> Learned codebook
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      (visual words)


Step 3: Feature Extraction (Bag-of-Words Encoding)
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Training     ‚îÇ     ‚îÇ 64 Visual    ‚îÇ
‚îÇ Image        ‚îÇ  +  ‚îÇ Prototypes   ‚îÇ
‚îÇ (124√ó124)    ‚îÇ     ‚îÇ (codebook)   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ                    ‚îÇ
       ‚Üì                    ‚îÇ
  Dense sampling            ‚îÇ
  8√ó8 patches               ‚îÇ
  (stride = 4)              ‚îÇ
       ‚îÇ                    ‚îÇ
       ‚Üì                    ‚îÇ
  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê           ‚îÇ
  ‚îÇ Patches:    ‚îÇ           ‚îÇ
  ‚îÇ P‚ÇÅ, P‚ÇÇ, ... ‚îÇ           ‚îÇ
  ‚îÇ P_N         ‚îÇ           ‚îÇ
  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò           ‚îÇ
       ‚îÇ                    ‚îÇ
       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                ‚Üì
        For each patch P·µ¢:
        Find BMU index k ‚àà {1,...,64}
                ‚Üì
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ Histogram [64D]  ‚îÇ
        ‚îÇ h[k] += 1        ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                ‚Üì
        L2 normalization
                ‚Üì
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ BoW Feature      ‚îÇ
        ‚îÇ (64D vector)     ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò


Step 4: SVM Classification (Supervised Learning)
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ BoW Features     ‚îÇ
‚îÇ (5,327 √ó 64D)    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Support Vector Machines (SVM)        ‚îÇ
‚îÇ                                        ‚îÇ
‚îÇ  One-vs-All Strategy:                 ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îÇ
‚îÇ  ‚îÇ SVM for   ‚îÇ  ‚îÇ SVM for   ‚îÇ  ...    ‚îÇ
‚îÇ  ‚îÇ class '0' ‚îÇ  ‚îÇ class '4' ‚îÇ         ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îÇ
‚îÇ                                        ‚îÇ
‚îÇ  7 binary classifiers                 ‚îÇ
‚îÇ  Linear or RBF kernel                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Trained Model    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò


‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     TESTING PIPELINE                            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Test Image ‚îÄ‚îÄ> Extract patches ‚îÄ‚îÄ> Find BMU for each
                                    ‚Üì
                            Build histogram ‚îÄ‚îÄ> SVM predict
                                    ‚Üì
                            Predicted class
```

---

## üîß Implementation Details

### 1. Patch Sampling

**Purpose**: Collect diverse local patterns for unsupervised learning

```matlab
function patches = extract_patches_for_som(data, num_samples, patch_size)
    % data: 4D array (H √ó W √ó C √ó N)
    % Returns: (num_samples √ó patch_size^2) matrix

    [H, W, ~, N] = size(data);
    patches = zeros(num_samples, patch_size * patch_size);

    for i = 1:num_samples
        % Random image
        img_idx = randi(N);
        img = squeeze(data(:, :, 1, img_idx));

        % Random location (ensure patch fits)
        y = randi([1, H - patch_size + 1]);
        x = randi([1, W - patch_size + 1]);

        % Extract and flatten patch
        patch = img(y:y+patch_size-1, x:x+patch_size-1);
        patches(i, :) = patch(:)' / 255;  % Normalize to [0,1]
    end
end
```

**Parameters**:
- `patch_size`: 8 (8√ó8 patches)
- `num_samples`: 50,000 - 100,000
- Sampling strategy: Uniform random from all training images

---

### 2. SOM Training

**Purpose**: Learn a topologically-organized codebook of visual prototypes

**Algorithm**: Competitive Learning with Neighborhood Updates

```matlab
function som_model = train_som(patches, grid_size, num_iterations)
    % Initialize SOM weights randomly from data distribution
    [M, D] = size(patches);  % M samples, D dimensions (64)
    num_neurons = grid_size(1) * grid_size(2);

    % Random initialization from data samples
    som_weights = patches(randperm(M, num_neurons), :);

    % Create 2D grid coordinates
    [grid_x, grid_y] = meshgrid(1:grid_size(1), 1:grid_size(2));
    neuron_coords = [grid_x(:), grid_y(:)];

    % Training parameters
    lr_init = 0.5;
    lr_final = 0.01;
    sigma_init = 3.0;
    sigma_final = 0.5;

    for iter = 1:num_iterations
        % Decay schedule
        t = iter / num_iterations;
        lr = lr_init * (1 - t) + lr_final * t;
        sigma = sigma_init * (1 - t) + sigma_final * t;

        % Random sample
        idx = randi(M);
        sample = patches(idx, :);

        % Find BMU (Best Matching Unit)
        distances = sum((som_weights - sample).^2, 2);
        [~, bmu_idx] = min(distances);

        % Update BMU and neighbors
        bmu_coord = neuron_coords(bmu_idx, :);
        for k = 1:num_neurons
            % Distance in grid space
            grid_dist = norm(neuron_coords(k, :) - bmu_coord);

            % Neighborhood function (Gaussian)
            h = exp(-grid_dist^2 / (2 * sigma^2));

            % Weight update
            som_weights(k, :) = som_weights(k, :) + ...
                lr * h * (sample - som_weights(k, :));
        end

        if mod(iter, 500) == 0
            fprintf('  SOM iteration %d/%d\n', iter, num_iterations);
        end
    end

    som_model = struct();
    som_model.weights = som_weights;
    som_model.grid_size = grid_size;
    som_model.patch_size = sqrt(D);
end
```

**Key Parameters**:
- Grid size: 8√ó8 = 64 neurons
- Iterations: 2000 - 5000
- Learning rate: 0.5 ‚Üí 0.01 (linear decay)
- Neighborhood radius: 3.0 ‚Üí 0.5 (linear decay)
- Neighborhood function: Gaussian

**Mathematical Formulation**:

Weight update rule:
$$w_k(t+1) = w_k(t) + \eta(t) \cdot h_{ck}(t) \cdot (x(t) - w_k(t))$$

Where:
- $w_k$: weight vector of neuron $k$
- $x$: input sample
- $\eta(t)$: learning rate at time $t$
- $h_{ck}(t)$: neighborhood function centered at BMU $c$

Neighborhood function:
$$h_{ck}(t) = \exp\left(-\frac{d_{ck}^2}{2\sigma(t)^2}\right)$$

Where:
- $d_{ck}$: grid distance between neuron $k$ and BMU $c$
- $\sigma(t)$: neighborhood radius at time $t$

---

### 3. Bag-of-Words Feature Extraction

**Purpose**: Encode each image as a histogram over visual words

```matlab
function features = extract_bow_features(data, som_model, stride)
    % data: 4D array (H √ó W √ó C √ó N)
    % Returns: (N √ó num_neurons) feature matrix

    [H, W, ~, N] = size(data);
    patch_size = som_model.patch_size;
    num_neurons = size(som_model.weights, 1);

    features = zeros(N, num_neurons);

    for i = 1:N
        img = double(squeeze(data(:, :, 1, i))) / 255;
        histogram = zeros(1, num_neurons);

        % Dense patch sampling with stride
        count = 0;
        for y = 1:stride:(H - patch_size + 1)
            for x = 1:stride:(W - patch_size + 1)
                % Extract patch
                patch = img(y:y+patch_size-1, x:x+patch_size-1);
                patch_vec = patch(:)';

                % Find BMU
                distances = sum((som_model.weights - patch_vec).^2, 2);
                [~, bmu_idx] = min(distances);

                % Accumulate in histogram
                histogram(bmu_idx) = histogram(bmu_idx) + 1;
                count = count + 1;
            end
        end

        % Normalize histogram (L2 norm)
        histogram = histogram / norm(histogram + 1e-10);
        features(i, :) = histogram;

        if mod(i, 500) == 0
            fprintf('  Processed %d/%d images\n', i, N);
        end
    end
end
```

**Parameters**:
- Stride: 4 (50% overlap between patches)
- Normalization: L2 norm for scale invariance

**Mathematical Formulation**:

For an image $I$, extract patches $\{p_1, p_2, ..., p_M\}$

For each patch $p_i$, find BMU:
$$c_i = \arg\min_k \|p_i - w_k\|^2$$

Build histogram:
$$h[k] = \sum_{i=1}^{M} \mathbb{1}[c_i = k]$$

L2 normalization:
$$\tilde{h} = \frac{h}{\|h\|_2 + \epsilon}$$

---

### 4. SVM Classification

**Purpose**: Multi-class classification on BoW features

Reuse the custom SVM implementation from initial Task 7.2 attempt:
- `trainLinearSVM.m`: SGD-based linear SVM training
- `predictSVM.m`: SVM prediction
- One-vs-All strategy for 7-class problem

**Parameters**:
- Kernel: Linear (for interpretability and speed)
- Regularization C: 1.0
- Optimizer: SGD with momentum (0.9)
- Learning rate: 0.01 ‚Üí 1e-5 (linear decay)
- Iterations: 1000

---

## üìÅ File Structure

```
src/
‚îú‚îÄ‚îÄ task7_2.m                          # Main training script
‚îú‚îÄ‚îÄ core/
‚îÇ   ‚îú‚îÄ‚îÄ som/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ train_som.m                # SOM training algorithm
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ find_bmu.m                 # Find best matching unit
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ som_forward.m              # Forward pass (BMU finding)
‚îÇ   ‚îú‚îÄ‚îÄ features/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ extract_patches.m          # Random patch sampling
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ extract_bow_features.m     # BoW feature extraction
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ normalize_features.m       # Feature normalization
‚îÇ   ‚îî‚îÄ‚îÄ network/
‚îÇ       ‚îú‚îÄ‚îÄ trainLinearSVM.m           # SVM training (reuse from previous)
‚îÇ       ‚îî‚îÄ‚îÄ predictSVM.m               # SVM prediction
‚îî‚îÄ‚îÄ utils/
    ‚îî‚îÄ‚îÄ visualization/
        ‚îú‚îÄ‚îÄ visualize_som_codebook.m   # Visualize learned prototypes
        ‚îú‚îÄ‚îÄ visualize_activation.m     # Visualize activation histograms
        ‚îî‚îÄ‚îÄ plot_results.m             # Standard result plots

output/task7_2/
‚îú‚îÄ‚îÄ som_model.mat                      # Trained SOM codebook
‚îú‚îÄ‚îÄ bow_features_train.mat             # BoW features (training set)
‚îú‚îÄ‚îÄ bow_features_test.mat              # BoW features (test set)
‚îú‚îÄ‚îÄ svm_models.mat                     # Trained SVM classifiers
‚îú‚îÄ‚îÄ predictions.mat                    # Test predictions
‚îú‚îÄ‚îÄ results.txt                        # Text summary
‚îÇ
‚îú‚îÄ‚îÄ figures/
‚îÇ   ‚îú‚îÄ‚îÄ som_codebook.png               # 8√ó8 grid of visual prototypes
‚îÇ   ‚îú‚îÄ‚îÄ som_codebook_detailed.png      # Individual prototypes with labels
‚îÇ   ‚îú‚îÄ‚îÄ activation_examples.png        # Sample images + histograms
‚îÇ   ‚îú‚îÄ‚îÄ confusion_matrix.png           # Normalized confusion matrix
‚îÇ   ‚îú‚îÄ‚îÄ per_class_accuracy.png         # Bar chart
‚îÇ   ‚îî‚îÄ‚îÄ misclassification_examples.png # Error analysis
```

---

## üéØ Training Script Outline

**File**: `src/task7_2.m`

```matlab
%% Task 7.2: SOM + Bag-of-Visual-Words + SVM Classification
clear all; close all;

% Add paths
addpath('core/som');
addpath('core/features');
addpath('core/network');
addpath('utils/visualization');

output_dir = '../output/task7_2/';
if ~exist(output_dir, 'dir'), mkdir(output_dir); end

fprintf('=== Task 7.2: SOM + BoW + SVM ===\n\n');

%% 1. Load Dataset
load('../data/train.mat');  % data_train, labels_train
load('../data/test.mat');   % data_test, labels_test

%% 2. Patch Sampling
fprintf('[1/5] Sampling patches for SOM training...\n');
num_patches = 50000;
patch_size = 8;
patches = extract_patches(data_train, num_patches, patch_size);
fprintf('  Sampled %d patches of size %dx%d\n\n', num_patches, patch_size, patch_size);

%% 3. Train SOM
fprintf('[2/5] Training SOM codebook...\n');
grid_size = [8, 8];  % 64 visual words
num_iterations = 2000;
som_model = train_som(patches, grid_size, num_iterations);
save([output_dir 'som_model.mat'], 'som_model');
fprintf('  SOM training complete\n\n');

% Visualize codebook
visualize_som_codebook(som_model, output_dir);

%% 4. Extract BoW Features
fprintf('[3/5] Extracting Bag-of-Words features...\n');
stride = 4;  % Dense sampling

fprintf('  Training set...\n');
features_train = extract_bow_features(data_train, som_model, stride);
save([output_dir 'bow_features_train.mat'], 'features_train');

fprintf('  Test set...\n');
features_test = extract_bow_features(data_test, som_model, stride);
save([output_dir 'bow_features_test.mat'], 'features_test');
fprintf('  Feature extraction complete\n\n');

%% 5. Train SVM
fprintf('[4/5] Training SVM classifiers (One-vs-All)...\n');
numClasses = 7;
class_names = {'0', '4', '7', '8', 'A', 'D', 'H'};
svm_models = cell(numClasses, 1);

tic;
for c = 1:numClasses
    fprintf('  Training SVM for class %s (%d/%d)...\n', class_names{c}, c, numClasses);

    % Binary labels
    labels_binary = double(labels_train == (c-1));
    labels_binary(labels_binary == 0) = -1;

    % Train SVM
    svm_models{c} = trainLinearSVM(features_train, labels_binary, ...
        'C', 1.0, 'MaxIter', 1000, 'Verbose', false);
end
training_time = toc;
save([output_dir 'svm_models.mat'], 'svm_models', 'class_names');
fprintf('  SVM training complete (%.2f seconds)\n\n', training_time);

%% 6. Evaluate
fprintf('[5/5] Evaluating on test set...\n');
% ... (prediction and evaluation code)

fprintf('\n=== Task 7.2 Complete ===\n');
```

---

## üìä Expected Performance

### Accuracy Prediction

Based on the Bag-of-Visual-Words literature and our dataset characteristics:

| Metric | Expected Range | Target |
|--------|----------------|--------|
| Overall Test Accuracy | 88% - 92% | 90% |
| Training Time | 5 - 10 min | 7 min |
| Per-class Accuracy (Digits) | 90% - 95% | 92% |
| Per-class Accuracy (Letters) | 85% - 90% | 88% |

### Comparison with CNN (Task 7.1)

| Aspect | CNN | SOM+BoW+SVM |
|--------|-----|-------------|
| Test Accuracy | 94.79% | ~90% (expected) |
| Training Time | 17.75 min | ~7 min |
| Parameters | ~50K | ~4K (64√ó64 SOM weights) |
| Feature Learning | Automatic (hierarchical) | Manual (BoW encoding) |
| Spatial Structure | Exploited (convolution) | Partially (local patches) |
| Interpretability | Low (black box) | High (visual prototypes) |

### Key Insights for Task 7.3 (Comparison)

**Advantages of SOM+BoW+SVM**:
1. **Faster training** (~2.5√ó faster than CNN)
2. **Highly interpretable** (can visualize learned visual words)
3. **Fewer parameters** (more efficient)
4. **Part-based representation** (robust to small deformations)

**Disadvantages**:
1. **Lower accuracy** (~5% gap from CNN)
2. **Manual feature design** (patch size, grid size, stride)
3. **No end-to-end optimization** (SOM and SVM trained separately)
4. **Limited spatial modeling** (bag-of-words discards spatial layout)

---

## üé® Visualization Plan

### 1. SOM Codebook Visualization

**Figure 1: Visual Prototypes Grid**
- 8√ó8 grid showing all 64 learned prototypes
- Each cell displays an 8√ó8 grayscale patch
- Title: "Learned Visual Words (SOM Codebook)"

```matlab
function visualize_som_codebook(som_model, output_dir)
    weights = som_model.weights;
    grid_size = som_model.grid_size;
    patch_size = som_model.patch_size;

    figure('Position', [100, 100, 800, 800]);
    for i = 1:prod(grid_size)
        subplot(grid_size(1), grid_size(2), i);
        patch = reshape(weights(i, :), patch_size, patch_size);
        imshow(patch, []);
        axis off;
    end
    sgtitle('Learned Visual Words (SOM Codebook)', 'FontSize', 14);
    saveas(gcf, [output_dir 'figures/som_codebook.png']);
end
```

### 2. Activation Histogram Examples

**Figure 2: Character Encoding**
- Show 4-6 sample images (2 per class, correct + misclassified)
- Below each image: 64-bin histogram showing activation distribution
- Demonstrates how different characters activate different visual words

### 3. Standard Performance Plots

Reuse visualization code from Task 7.1:
- **Confusion Matrix**: Normalized with counts + percentages
- **Per-class Accuracy**: Bar chart
- **Misclassification Examples**: 3√ó4 grid of error cases

---

## üìù Report Structure

### Section: Task 7.2 - Non-CNN Method

#### 7.2.1 Introduction
- Motivation: Traditional ML approach using Part 2 methods
- Overview: SOM (unsupervised) + SVM (supervised) combination
- Bag-of-Visual-Words concept
- Rationale: Local patch features for character recognition

#### 7.2.2 Method

**Self-Organizing Map (SOM)**
- Algorithm description
- Training procedure
- Mathematical formulation (competitive learning, neighborhood function)
- Hyperparameters

**Bag-of-Visual-Words Feature Encoding**
- Patch extraction strategy
- Codebook lookup (BMU finding)
- Histogram construction
- Normalization

**SVM Classification**
- One-vs-All multi-class strategy
- Linear kernel choice
- Training algorithm (SGD)

#### 7.2.3 Results
- Overall performance (accuracy, training time)
- Per-class accuracy table + bar chart
- Confusion matrix analysis
- Misclassification patterns

**Visualization Analysis**:
- Learned visual prototypes (what patterns did SOM discover?)
- Activation histogram comparison between classes
- Example: "Class '0' strongly activates circular edge prototypes, while 'H' activates vertical/horizontal edge prototypes"

#### 7.2.4 Discussion

**Method Characteristics**:
- Part-based representation vs. holistic features
- Effect of codebook size on performance
- Interpretability advantage over deep learning

**Comparison Teaser** (detailed in Task 7.3):
- SOM+BoW: Faster, interpretable, but lower accuracy
- CNN: Slower, black-box, but superior performance
- Trade-off: Efficiency vs. Accuracy

---

## ‚ö†Ô∏è Potential Challenges & Solutions

### Challenge 1: SOM Convergence
**Issue**: SOM may not converge well with poor initialization
**Solution**:
- Initialize weights from random data samples (not pure random)
- Use sufficient training iterations (2000+)
- Monitor quantization error during training

### Challenge 2: Codebook Size Selection
**Issue**: Unclear optimal grid size (6√ó6? 8√ó8? 10√ó10?)
**Solution**:
- Start with 8√ó8 = 64 (standard BoW size)
- If time permits, compare {6√ó6, 8√ó8, 10√ó10}
- Report: "64 visual words provides good balance between expressiveness and efficiency"

### Challenge 3: Low Accuracy
**Issue**: If accuracy < 85%, may look weak compared to CNN
**Solution**:
- Emphasize **interpretability** and **efficiency** advantages
- Multi-scale BoW: Extract histograms at multiple patch sizes (8√ó8, 12√ó12, 16√ó16), concatenate features
- Longer SOM training or more patches

### Challenge 4: Implementation Time
**Issue**: SOM training + feature extraction may take longer than expected
**Solution**:
- Parallelize feature extraction if possible
- Cache intermediate results (patches, SOM model, features)
- Start with smaller num_patches (30K) for quick prototype

---

## ‚úÖ Success Criteria

### Minimum Requirements (Must Achieve)
- [ ] SOM training converges successfully
- [ ] Test accuracy ‚â• 85%
- [ ] Training completes in < 15 minutes
- [ ] All visualizations generate correctly
- [ ] Code runs without errors

### Target Goals (Ideal)
- [ ] Test accuracy ‚â• 90%
- [ ] Training time ‚â§ 10 minutes
- [ ] Clear visual prototypes (interpretable)
- [ ] Per-class accuracy: digits > 90%, letters > 85%

### Bonus (If Time Permits)
- [ ] Multi-scale BoW (multiple patch sizes)
- [ ] Codebook size comparison experiment
- [ ] RBF kernel SVM comparison
- [ ] t-SNE visualization of BoW feature space

---

## üìÖ Implementation Timeline

**Estimated Total Time**: 3-4 hours

| Task | Time | Status |
|------|------|--------|
| 1. SOM training implementation | 45 min | ‚è≥ Pending |
| 2. BoW feature extraction | 30 min | ‚è≥ Pending |
| 3. SVM integration (reuse code) | 15 min | ‚è≥ Pending |
| 4. Run training pipeline | 10 min | ‚è≥ Pending |
| 5. Generate visualizations | 30 min | ‚è≥ Pending |
| 6. Write report section | 60 min | ‚è≥ Pending |
| **Total** | **3h 10min** | |

---

## üîç References

**Theoretical Foundation**:
1. Kohonen, T. (1990). "The self-organizing map". Proceedings of the IEEE.
2. Sivic, J. & Zisserman, A. (2003). "Video Google: A text retrieval approach to object matching in videos". ICCV.
3. Csurka, G. et al. (2004). "Visual categorization with bags of keypoints". ECCV Workshop.
4. Cortes, C. & Vapnik, V. (1995). "Support-vector networks". Machine Learning.

**Course Material**:
- ME5411 Part 2: Self-Organizing Maps (SOM)
- ME5411 Part 2: Support Vector Machines (SVM)

---

## üìå Notes

- This plan prioritizes **course compliance** (100% Part 2 methods) over maximum accuracy
- The method is **innovative** (SOM for codebook learning is less common than k-means) and shows deep understanding
- **Interpretability** is a major selling point: can visualize what the model learned
- Implementation is **modular**: each component (SOM, BoW, SVM) is independent and testable

---

**Last Updated**: November 3, 2025
**Status**: Ready for implementation
**Confidence Level**: 85% success probability for ‚â•90% accuracy
